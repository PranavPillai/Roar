{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the imports\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "from numpy import random, delete\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "import mord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14993, 24)\n",
      "(13724, 24)\n",
      "(7480, 24)\n",
      "(6244, 24)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# pre-processing, separate by animal type\n",
    "train = pd.read_csv('train.csv')\n",
    "print(train.shape)\n",
    "train = train.dropna()\n",
    "print(train.shape)\n",
    "\n",
    "dogs_df = train.loc[train['Type'] == 1]\n",
    "cats_df = train.loc[train['Type'] == 2]\n",
    "print(dogs_df.shape)\n",
    "print(cats_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7480, 12)\n",
      "(6244, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AdoptionSpeed\n",
       "0              3\n",
       "1              2\n",
       "2              2\n",
       "3              3\n",
       "4              1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select certain features of interest in dogs and cats\n",
    "    \n",
    "# want Age, Breed1, Gender, Color1, Color2?, Color3?, MaturitySize, FurLength, Vaccinated, Dewormed, Sterilized, Health, Fee?, State, PhotoAmt\n",
    "dogs_sub_df = dogs_df[['Age', 'Breed1', 'Gender', 'Color1', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized', 'Health', 'State', 'PhotoAmt']].reset_index(drop=True)\n",
    "cats_sub_df = cats_df[['Age', 'Breed1', 'Gender', 'Color1', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized', 'Health', 'State', 'PhotoAmt']].reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(dogs_sub_df.shape)\n",
    "print(cats_sub_df.shape)\n",
    "\n",
    "dogs_labels_df = dogs_df[['AdoptionSpeed']].reset_index(drop=True)\n",
    "cats_labels_df = cats_df[['AdoptionSpeed']].reset_index(drop=True)\n",
    "\n",
    "dogs_labels_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to change to binary class (adopted w/in 100 days 1, not 0)\n",
    "def adoptSpeed(x):\n",
    "    if x == 4: # might be too rare of a bench mark if it's 2/7 ?\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def adoptSpeedBins(x):\n",
    "    if x == 0 or x == 1: \n",
    "        return 0\n",
    "    elif x == 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "dogs_labels_2_df = dogs_labels_df.applymap(adoptSpeed)\n",
    "dogs_labels_bins_df = dogs_labels_df.applymap(adoptSpeedBins)\n",
    "cats_labels_2_df = cats_labels_df.applymap(adoptSpeed)\n",
    "cats_labels_bins_df = cats_labels_df.applymap(adoptSpeedBins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4488, 12)\n",
      "(7480, 1)\n",
      "(4488, 12)\n",
      "(2992, 12)\n",
      "(2992, 1)\n",
      "(3746, 12)\n",
      "(2498, 12)\n",
      "(2498, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# PROCESSING INTO TRAIN AND TEST SETS \n",
    "\n",
    "dogs_train_orig = dogs_sub_df.values\n",
    "dogs_train_labels = dogs_labels_df.values\n",
    "dogs_train_2_labels = dogs_labels_2_df.values\n",
    "dogs_train_bins_labels = dogs_labels_bins_df.values\n",
    "\n",
    "cats_train_orig = cats_sub_df.values\n",
    "cats_train_labels = cats_labels_df.values\n",
    "cats_train_2_labels = cats_labels_2_df.values\n",
    "cats_train_bins_labels = cats_labels_bins_df.values\n",
    "\n",
    "dogs_train, dogs_test, dogs_train_labels, dogs_test_labels = train_test_split(dogs_train_orig, dogs_train_labels, test_size=0.40)\n",
    "print(dogs_train.shape)\n",
    "print(dogs_train_2_labels.shape)\n",
    "dogs_train_2, dogs_test_2, dogs_train_2_labels, dogs_test_2_labels = train_test_split(dogs_train_orig, dogs_train_2_labels, test_size=0.40)\n",
    "dogs_train_bins, dogs_test_bins, dogs_train_bins_labels, dogs_test_bins_labels = train_test_split(dogs_train_orig, dogs_train_bins_labels, test_size=0.40)\n",
    "\n",
    "\n",
    "cats_train, cats_test, cats_train_labels, cats_test_labels = train_test_split(cats_train_orig, cats_train_labels, test_size=0.40)\n",
    "cats_train_2, cats_test_2, cats_train_2_labels, cats_test_2_labels = train_test_split(cats_train_orig, cats_train_2_labels, test_size=0.40)\n",
    "cats_train_bins, cats_test_bins, cats_train_bins_labels, cats_test_bins_labels = train_test_split(cats_train_orig, cats_train_bins_labels, test_size=0.40)\n",
    "\n",
    "scaler_d = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler_d.fit(dogs_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "dogs_train = scaler_d.transform(dogs_train)\n",
    "dogs_test = scaler_d.transform(dogs_test)\n",
    "\n",
    "scaler_d2 = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler_d2.fit(dogs_train_2)\n",
    "# Apply transform to both the training set and the test set.\n",
    "dogs_train_2 = scaler_d2.transform(dogs_train_2)\n",
    "dogs_test_2 = scaler_d2.transform(dogs_test_2)\n",
    "\n",
    "scaler_dbins = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler_dbins.fit(dogs_train_bins)\n",
    "# Apply transform to both the training set and the test set.\n",
    "dogs_train_bins = scaler_dbins.transform(dogs_train_bins)\n",
    "dogs_test_bins = scaler_dbins.transform(dogs_test_bins)\n",
    "\n",
    "scaler_c = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler_c.fit(cats_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "cats_train = scaler_c.transform(cats_train)\n",
    "cats_test = scaler_c.transform(cats_test)\n",
    "\n",
    "scaler_c2 = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler_c2.fit(cats_train_2)\n",
    "# Apply transform to both the training set and the test set.\n",
    "cats_train_2 = scaler_c2.transform(cats_train_2)\n",
    "cats_test_2 = scaler_c2.transform(cats_test_2)\n",
    "\n",
    "scaler_cbins = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler_cbins.fit(cats_train_bins)\n",
    "# Apply transform to both the training set and the test set.\n",
    "cats_train_bins = scaler_cbins.transform(cats_train_bins)\n",
    "cats_test_bins = scaler_cbins.transform(cats_test_bins)\n",
    "\n",
    "\n",
    "print(dogs_train.shape)\n",
    "print(dogs_test.shape)\n",
    "print(dogs_test_labels.shape)\n",
    "\n",
    "print(cats_train.shape)\n",
    "print(cats_test.shape)\n",
    "print(cats_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of LogisticAT on Training Data 0.9133244206773619\n",
      "Mean Absolute Error of LogisticAT on Test Data 0.9087566844919787\n",
      "Mean Absolute Error of LogisticIT on Training Data 1.1045008912655971\n",
      "Mean Absolute Error of LogisticIT on Test Data 1.1620989304812834\n",
      "Mean Absolute Error of LogisticSE on Training Data 0.9126559714795008\n",
      "Mean Absolute Error of LogisticSE on Test Data 0.9207887700534759\n"
     ]
    }
   ],
   "source": [
    "# computed on full label set DOGS\n",
    "\n",
    "# plot result vectors...\n",
    "\n",
    "clf = mord.LogisticAT(alpha=1) # absolute loss - \n",
    "clf.fit(dogs_train, dogs_train_labels.ravel())\n",
    "print('Mean Absolute Error of LogisticAT on Training Data %s' % metrics.mean_absolute_error(clf.predict(dogs_train), dogs_train_labels))\n",
    "print('Mean Absolute Error of LogisticAT on Test Data %s' % metrics.mean_absolute_error(clf.predict(dogs_test), dogs_test_labels))\n",
    "\n",
    "clf2 = mord.LogisticIT(alpha=1) # 0-1 loss\n",
    "clf2.fit(dogs_train, dogs_train_labels.ravel())\n",
    "print('Mean Absolute Error of LogisticIT on Training Data %s' % metrics.mean_absolute_error(clf2.predict(dogs_train), dogs_train_labels))\n",
    "print('Mean Absolute Error of LogisticIT on Test Data %s' % metrics.mean_absolute_error(clf2.predict(dogs_test), dogs_test_labels))\n",
    "\n",
    "clf3 = mord.LogisticSE(alpha=1) # squared error\n",
    "clf3.fit(dogs_train, dogs_train_labels.ravel())\n",
    "print('Mean Absolute Error of LogisticSE on Training Data %s' % metrics.mean_absolute_error(clf3.predict(dogs_train), dogs_train_labels))\n",
    "print('Mean Absolute Error of LogisticSE on Test Data %s' % metrics.mean_absolute_error(clf3.predict(dogs_test), dogs_test_labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of LogisticAT on Training Data 0.9684997330485852\n",
      "Mean Absolute Error of LogisticAT on Test Data 0.9719775820656525\n",
      "Mean Absolute Error of LogisticIT on Training Data 1.120928990923652\n",
      "Mean Absolute Error of LogisticIT on Test Data 1.1120896717373898\n",
      "Mean Absolute Error of LogisticSE on Trianing Data 0.9698344901227977\n",
      "Mean Absolute Error of LogisticSE on Test Data 0.9711769415532426\n"
     ]
    }
   ],
   "source": [
    "# CATS\n",
    "\n",
    "clf = mord.LogisticAT(alpha=1)\n",
    "clf.fit(cats_train, cats_train_labels.ravel())\n",
    "print('Mean Absolute Error of LogisticAT on Training Data %s' % metrics.mean_absolute_error(clf.predict(cats_train), cats_train_labels))\n",
    "print('Mean Absolute Error of LogisticAT on Test Data %s' % metrics.mean_absolute_error(clf.predict(cats_test), cats_test_labels))\n",
    "\n",
    "\n",
    "clf2 = mord.LogisticIT(alpha=1)\n",
    "clf2.fit(cats_train, cats_train_labels.ravel())\n",
    "print('Mean Absolute Error of LogisticIT on Training Data %s' % metrics.mean_absolute_error(clf2.predict(cats_train), cats_train_labels))\n",
    "print('Mean Absolute Error of LogisticIT on Test Data %s' % metrics.mean_absolute_error(clf2.predict(cats_test), cats_test_labels))\n",
    "\n",
    "\n",
    "clf3 = mord.LogisticSE(alpha=1)\n",
    "clf3.fit(cats_train, cats_train_labels.ravel())\n",
    "print('Mean Absolute Error of LogisticSE on Trianing Data %s' % metrics.mean_absolute_error(clf3.predict(cats_train), cats_train_labels))\n",
    "print('Mean Absolute Error of LogisticSE on Test Data %s' % metrics.mean_absolute_error(clf3.predict(cats_test), cats_test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0\n",
       "7   1\n",
       "8   1\n",
       "10  1\n",
       "12  1\n",
       "16  1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.DataFrame(dogs_train_labels)\n",
    "temp_sub = temp_df.loc[temp_df[0]==1] #3212 rows out of 4487 hmmm\n",
    "temp_sub.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy (before cv):  0.7219251336898396\n",
      "test accuracy (before cv):  0.7212566844919787\n",
      "0.48693596075319\n",
      "train accuracy (after cv):  0.7208110516934046\n",
      "test accuracy (after cv):  0.7219251336898396\n",
      "0.4864566236496851\n",
      "{'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# DOGS\n",
    "# output for binary classification\n",
    "# before cv\n",
    "logistic = linear_model.LogisticRegression(solver='lbfgs', multi_class='ovr')\n",
    "logistic.fit(dogs_train_2,dogs_train_2_labels.ravel())\n",
    "\n",
    "dogs_test_labels = dogs_test_2_labels.reshape(dogs_test_2_labels.shape[0],)\n",
    "train_pred = logistic.predict(dogs_train_2)\n",
    "\n",
    "adopt_pred = logistic.predict(dogs_test_2)\n",
    "\n",
    "train_accuracy = metrics.accuracy_score(dogs_train_2_labels, train_pred)\n",
    "print('train accuracy (before cv): ', train_accuracy)\n",
    "\n",
    "test_accuracy = metrics.accuracy_score(dogs_test_2_labels, adopt_pred)\n",
    "print('test accuracy (before cv): ', test_accuracy)\n",
    "\n",
    "f1 = f1_score(dogs_test_2_labels, adopt_pred, average='macro')\n",
    "print(f1)\n",
    "\n",
    "\n",
    "# after cv\n",
    "log = linear_model.LogisticRegression(solver='lbfgs', multi_class='ovr')\n",
    "parameters = {'C':[0.1, 1, 5, 10]}\n",
    "clf = GridSearchCV(log, parameters, cv=5)\n",
    "clf.fit(dogs_train_2,dogs_train_2_labels.ravel())\n",
    "\n",
    "train_pred_cv = clf.predict(dogs_train_2)\n",
    "adopt_pred_cv = clf.predict(dogs_test_2)\n",
    "lr_train_accuracy_dogs = clf.score(dogs_train_2, dogs_train_2_labels)\n",
    "lr_test_accuracy_dogs = clf.score(dogs_test_2, dogs_test_2_labels)\n",
    "\n",
    "print('train accuracy (after cv): ', lr_train_accuracy_dogs)\n",
    "print('test accuracy (after cv): ', lr_test_accuracy_dogs)\n",
    "lr_f1_cv_dogs = f1_score(dogs_test_2_labels, adopt_pred_cv, average='macro')\n",
    "print(lr_f1_cv_dogs)\n",
    "print(clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy (before cv):  0.7330485851575014\n",
      "test accuracy (before cv):  0.7526020816653323\n",
      "0.4713716305062459\n",
      "train accuracy (after cv):  0.7335824879871863\n",
      "test accuracy (after cv):  0.7526020816653323\n",
      "0.4713716305062459\n",
      "{'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# CATS\n",
    "# output for binary classification\n",
    "# before cv\n",
    "logistic = linear_model.LogisticRegression(solver='lbfgs', multi_class='ovr')\n",
    "logistic.fit(cats_train_2,cats_train_2_labels.ravel())\n",
    "\n",
    "cats_test_labels = cats_test_2_labels.reshape(cats_test_2_labels.shape[0],)\n",
    "train_pred = logistic.predict(cats_train_2)\n",
    "\n",
    "adopt_pred = logistic.predict(cats_test_2)\n",
    "\n",
    "train_accuracy = metrics.accuracy_score(cats_train_2_labels, train_pred)\n",
    "print('train accuracy (before cv): ', train_accuracy)\n",
    "\n",
    "test_accuracy = metrics.accuracy_score(cats_test_2_labels, adopt_pred)\n",
    "print('test accuracy (before cv): ', test_accuracy)\n",
    "\n",
    "f1 = f1_score(cats_test_2_labels, adopt_pred, average='macro')\n",
    "print(f1)\n",
    "\n",
    "\n",
    "# after cv\n",
    "log = linear_model.LogisticRegression(solver='lbfgs', multi_class='ovr')\n",
    "parameters = {'C':[0.1, 1, 5, 10]}\n",
    "clf = GridSearchCV(log, parameters, cv=5)\n",
    "clf.fit(cats_train_2,cats_train_2_labels.ravel())\n",
    "\n",
    "train_pred_cv = clf.predict(cats_train_2)\n",
    "adopt_pred_cv = clf.predict(cats_test_2)\n",
    "lr_train_accuracy_cats = clf.score(cats_train_2, cats_train_2_labels)\n",
    "lr_test_accuracy_cats = clf.score(cats_test_2, cats_test_2_labels)\n",
    "\n",
    "print('train accuracy (after cv): ', lr_train_accuracy_cats)\n",
    "print('test accuracy (after cv): ', lr_test_accuracy_cats)\n",
    "lr_f1_cats = f1_score(cats_test_2_labels, adopt_pred_cv, average='macro')\n",
    "print(lr_f1_cats)\n",
    "print(clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy (after cv):  0.7176916221033868\n",
      "test accuracy (after cv):  0.7185828877005348\n",
      "0.46306880183326055\n",
      "{'alpha': 1, 'solver': 'sparse_cg'}\n"
     ]
    }
   ],
   "source": [
    "# TRAINING VIA Ridge Regression Classifier - DOGS\n",
    "rc = RidgeClassifier()\n",
    "parameters = {'alpha':[0.01, 0.1, 1, 10], 'solver' : ('auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga')}\n",
    "clf = GridSearchCV(rc, parameters, cv=5)\n",
    "\n",
    "clf.fit(dogs_train_2,dogs_train_2_labels.ravel())\n",
    "\n",
    "train_pred_cv = clf.predict(dogs_train_2)\n",
    "adopt_pred_cv = clf.predict(dogs_test_2)\n",
    "rc_train_accuracy_dogs = clf.score(dogs_train_2, dogs_train_2_labels)\n",
    "rc_test_accuracy_dogs = clf.score(dogs_test_2, dogs_test_2_labels)\n",
    "\n",
    "print('train accuracy (after cv): ', rc_train_accuracy_dogs)\n",
    "print('test accuracy (after cv): ', rc_test_accuracy_dogs)\n",
    "rc_f1_dogs = f1_score(dogs_test_2_labels, adopt_pred_cv, average='macro')\n",
    "print(rc_f1_dogs)\n",
    "print(clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy (after cv):  0.7325146823278164\n",
      "test accuracy (after cv):  0.7522017614091273\n",
      "0.45758953079622255\n",
      "{'alpha': 0.01, 'solver': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "# TRAINING VIA Ridge Regression Classifier - CATS\n",
    "rc = RidgeClassifier()\n",
    "parameters = {'alpha':[0.01, 0.1, 1, 10], 'solver' : ('auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga')}\n",
    "clf = GridSearchCV(rc, parameters, cv=5)\n",
    "\n",
    "clf.fit(cats_train_2,cats_train_2_labels.ravel())\n",
    "\n",
    "train_pred_cv = clf.predict(cats_train_2)\n",
    "adopt_pred_cv = clf.predict(cats_test_2)\n",
    "rc_train_accuracy_cats = clf.score(cats_train_2, cats_train_2_labels)\n",
    "rc_test_accuracy_cats = clf.score(cats_test_2, cats_test_2_labels)\n",
    "\n",
    "print('train accuracy (after cv): ', rc_train_accuracy_cats)\n",
    "print('test accuracy (after cv): ', rc_test_accuracy_cats)\n",
    "rc_f1_cats = f1_score(cats_test_2_labels, adopt_pred_cv, average='macro')\n",
    "print(rc_f1_cats)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy (after cv):  0.7818627450980392\n",
      "test accuracy (after cv):  0.7195855614973262\n",
      "0.6086489570755867\n",
      "{'n_neighbors': 7}\n"
     ]
    }
   ],
   "source": [
    "# TRAINING VIA NEAREST NEIGHBORS - DOGS\n",
    "\n",
    "neigh = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors':(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)}\n",
    "clf = GridSearchCV(neigh, parameters, cv=5)\n",
    "\n",
    "clf.fit(dogs_train_2,dogs_train_2_labels.ravel())\n",
    "\n",
    "train_pred_cv = clf.predict(dogs_train_2)\n",
    "adopt_pred_cv = clf.predict(dogs_test_2)\n",
    "nn_train_accuracy_dogs = clf.score(dogs_train_2, dogs_train_2_labels)\n",
    "nn_test_accuracy_dogs = clf.score(dogs_test_2, dogs_test_2_labels)\n",
    "\n",
    "print('train accuracy (after cv): ', nn_train_accuracy_dogs)\n",
    "print('test accuracy (after cv): ', nn_test_accuracy_dogs)\n",
    "nn_f1_dogs = f1_score(dogs_test_2_labels, adopt_pred_cv, average='macro')\n",
    "print(nn_f1_dogs)\n",
    "print(clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy (after cv):  0.7688200747463961\n",
      "test accuracy (after cv):  0.7413931144915933\n",
      "0.5711107000399731\n",
      "{'n_neighbors': 9}\n"
     ]
    }
   ],
   "source": [
    "# TRAINING VIA NEAREST NEIGHBORS - CATS\n",
    "\n",
    "neigh = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors':(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)}\n",
    "clf = GridSearchCV(neigh, parameters, cv=5)\n",
    "\n",
    "clf.fit(cats_train_2,cats_train_2_labels.ravel())\n",
    "\n",
    "train_pred_cv = clf.predict(cats_train_2)\n",
    "adopt_pred_cv = clf.predict(cats_test_2)\n",
    "nn_train_accuracy_cats = clf.score(cats_train_2, cats_train_2_labels)\n",
    "nn_test_accuracy_cats = clf.score(cats_test_2, cats_test_2_labels)\n",
    "\n",
    "print('train accuracy (after cv): ', nn_train_accuracy_cats)\n",
    "print('test accuracy (after cv): ', nn_test_accuracy_cats)\n",
    "nn_f1_cats = f1_score(cats_test_2_labels, adopt_pred_cv, average='macro')\n",
    "print(nn_f1_cats)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy (after cv):  0.9750445632798574\n",
      "test accuracy (after cv):  0.6908422459893048\n",
      "0.6219815102027157\n",
      "{'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "# TRAINING VIA DECISION TREES - DOGS\n",
    "dec_tree = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "parameters = {'criterion':('gini', 'entropy')}\n",
    "clf = GridSearchCV(dec_tree, parameters, cv=5)\n",
    "\n",
    "clf.fit(dogs_train_2,dogs_train_2_labels.ravel())\n",
    "\n",
    "train_pred_cv = clf.predict(dogs_train_2)\n",
    "adopt_pred_cv = clf.predict(dogs_test_2)\n",
    "dt_train_accuracy_dogs = clf.score(dogs_train_2, dogs_train_2_labels)\n",
    "dt_test_accuracy_dogs = clf.score(dogs_test_2, dogs_test_2_labels)\n",
    "\n",
    "print('train accuracy (after cv): ', dt_train_accuracy_dogs)\n",
    "print('test accuracy (after cv): ', dt_test_accuracy_dogs)\n",
    "dt_f1_dogs = f1_score(dogs_test_2_labels, adopt_pred_cv, average='macro')\n",
    "print(dt_f1_dogs)\n",
    "print(clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy (after cv):  0.9919914575547251\n",
      "test accuracy (after cv):  0.677742193755004\n",
      "0.578309096837984\n",
      "{'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "# TRAINING VIA DECISION TREES - CATS\n",
    "dec_tree = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "parameters = {'criterion':('gini', 'entropy')}\n",
    "clf = GridSearchCV(dec_tree, parameters, cv=5)\n",
    "\n",
    "clf.fit(cats_train_2,cats_train_2_labels.ravel())\n",
    "\n",
    "train_pred_cv = clf.predict(cats_train_2)\n",
    "adopt_pred_cv = clf.predict(cats_test_2)\n",
    "dt_train_accuracy_cats = clf.score(cats_train_2, cats_train_2_labels)\n",
    "dt_test_accuracy_cats = clf.score(cats_test_2, cats_test_2_labels)\n",
    "\n",
    "print('train accuracy (after cv): ', dt_train_accuracy_cats)\n",
    "print('test accuracy (after cv): ', dt_test_accuracy_cats)\n",
    "dt_f1_cats = f1_score(cats_test_2_labels, adopt_pred_cv, average='macro')\n",
    "print(dt_f1_cats)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy (after cv):  0.7181372549019608\n",
      "test accuracy (after cv):  0.7192513368983957\n",
      "0.4652846036994013\n",
      "{'C': 0.1, 'loss': 'squared_hinge'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# TRAINING VIA LINEAR SVM - DOGS\n",
    "svm = LinearSVC(random_state=0, tol=1e-5, max_iter=10000)\n",
    "\n",
    "parameters = {'loss':('hinge', 'squared_hinge'), 'C':[0.1, 0.5, 1, 5, 10]}\n",
    "clf = GridSearchCV(svm, parameters, cv=5)\n",
    "\n",
    "clf.fit(dogs_train_2,dogs_train_2_labels.ravel())\n",
    "\n",
    "train_pred_cv = clf.predict(dogs_train_2)\n",
    "adopt_pred_cv = clf.predict(dogs_test_2)\n",
    "svm_train_accuracy_dogs = clf.score(dogs_train_2, dogs_train_2_labels)\n",
    "svm_test_accuracy_dogs = clf.score(dogs_test_2, dogs_test_2_labels)\n",
    "\n",
    "print('train accuracy (after cv): ', svm_train_accuracy_dogs)\n",
    "print('test accuracy (after cv): ', svm_test_accuracy_dogs)\n",
    "svm_f1_dogs = f1_score(dogs_test_2_labels, adopt_pred_cv, average='macro')\n",
    "print(svm_f1_dogs)\n",
    "print(clf.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy (after cv):  0.7354511478910838\n",
      "test accuracy (after cv):  0.754603682946357\n",
      "0.4300707278120009\n",
      "{'C': 0.1, 'loss': 'hinge'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# TRAINING VIA LINEAR SVM - CATS\n",
    "svm = LinearSVC(random_state=0, tol=1e-5, max_iter=10000)\n",
    "\n",
    "parameters = {'loss':('hinge', 'squared_hinge'), 'C':[0.1, 0.5, 1, 5, 10]}\n",
    "clf = GridSearchCV(svm, parameters, cv=5)\n",
    "\n",
    "clf.fit(cats_train_2,cats_train_2_labels.ravel())\n",
    "\n",
    "train_pred_cv = clf.predict(cats_train_2)\n",
    "adopt_pred_cv = clf.predict(cats_test_2)\n",
    "svm_train_accuracy_cats = clf.score(cats_train_2, cats_train_2_labels)\n",
    "svm_test_accuracy_cats = clf.score(cats_test_2, cats_test_2_labels)\n",
    "\n",
    "print('train accuracy (after cv): ', svm_train_accuracy_cats)\n",
    "print('test accuracy (after cv): ', svm_test_accuracy_cats)\n",
    "svm_f1_cats = f1_score(cats_test_2_labels, adopt_pred_cv, average='macro')\n",
    "print(svm_f1_cats)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempts at ensemble - voting classifier\n",
    "\n",
    "clf1 = linear_model.LogisticRegression(solver='lbfgs', multi_class='ovr')\n",
    "clf2 = DecisionTreeClassifier(random_state=0)\n",
    "clf3 = KNeighborsClassifier()\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('dtree', clf2), ('kneigh', clf3)], voting='hard')\n",
    "\n",
    "params = {'lr__C': [1.0, 100.0], 'kneigh__n_neighbors': [2, 3, 4, 5]}\n",
    "\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "grid = grid.fit(dogs_train_2, dogs_train_2_labels.ravel())\n",
    "\n",
    "train_pred_cv = grid.predict(dogs_train_2)\n",
    "adopt_pred_cv = grid.predict(dogs_test_2)\n",
    "train_accuracy_cv = grid.score(dogs_train_2, dogs_train_2_labels)\n",
    "test_accuracy_cv = grid.score(dogs_test_2, dogs_test_2_labels)\n",
    "\n",
    "print('train accuracy (after cv): ', train_accuracy_cv)\n",
    "print('test accuracy (after cv): ', test_accuracy_cv)\n",
    "f1_cv = f1_score(dogs_test_2_labels, adopt_pred_cv, average='macro')\n",
    "print(f1_cv)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression train accuracy:  0.7208110516934046\n",
      "logistic regression test accuracy:  0.7219251336898396\n",
      "logistic regression f1 score:  0.4864566236496851\n",
      "ridge regression train accuracy:  0.7176916221033868\n",
      "ridge regression test accuracy:  0.7185828877005348\n",
      "ridge regression f1 score:  0.46306880183326055\n",
      "nearest neighbors train accuracy:  0.7818627450980392\n",
      "nearest neighbors test accuracy:  0.7195855614973262\n",
      "nearest neighbors f1 score:  0.6086489570755867\n",
      "decision tree train accuracy:  0.9750445632798574\n",
      "decision tree test accuracy:  0.6908422459893048\n",
      "decision tree f1 score:  0.6219815102027157\n",
      "svm train accuracy:  0.7181372549019608\n",
      "svm test accuracy:  0.7192513368983957\n",
      "svm f1 score:  0.4652846036994013\n",
      "DOGS: Choose logistic regression for highest test accuracy 0.7219 (f1 score 0.486), choose neighest neighbors for best balance w/ f1 score 0.719 test accuracy (f1 score 0.609)\n"
     ]
    }
   ],
   "source": [
    "# COMPARE MODELS FOR DOGS\n",
    "print(\"logistic regression train accuracy: \", lr_train_accuracy_dogs)\n",
    "print(\"logistic regression test accuracy: \", lr_test_accuracy_dogs)\n",
    "print(\"logistic regression f1 score: \", lr_f1_cv_dogs)\n",
    "\n",
    "print(\"ridge regression train accuracy: \", rc_train_accuracy_dogs)\n",
    "print(\"ridge regression test accuracy: \", rc_test_accuracy_dogs)\n",
    "print(\"ridge regression f1 score: \", rc_f1_dogs)\n",
    "\n",
    "print(\"nearest neighbors train accuracy: \", nn_train_accuracy_dogs)\n",
    "print(\"nearest neighbors test accuracy: \", nn_test_accuracy_dogs)\n",
    "print(\"nearest neighbors f1 score: \", nn_f1_dogs)\n",
    "\n",
    "print(\"decision tree train accuracy: \", dt_train_accuracy_dogs)\n",
    "print(\"decision tree test accuracy: \", dt_test_accuracy_dogs)\n",
    "print(\"decision tree f1 score: \", dt_f1_dogs)\n",
    "\n",
    "print(\"svm train accuracy: \", svm_train_accuracy_dogs)\n",
    "print(\"svm test accuracy: \", svm_test_accuracy_dogs)\n",
    "print(\"svm f1 score: \", svm_f1_dogs)\n",
    "\n",
    "print(\"DOGS: Choose logistic regression for highest test accuracy 0.7219 (f1 score 0.486), choose neighest neighbors for best balance w/ f1 score 0.719 test accuracy (f1 score 0.609)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression train accuracy:  0.7335824879871863\n",
      "logistic regression test accuracy:  0.7526020816653323\n",
      "logistic regression f1 score:  0.4713716305062459\n",
      "ridge regression train accuracy:  0.7325146823278164\n",
      "ridge regression test accuracy:  0.7522017614091273\n",
      "ridge regression f1 score:  0.45758953079622255\n",
      "nearest neighbors train accuracy:  0.7688200747463961\n",
      "nearest neighbors test accuracy:  0.7413931144915933\n",
      "nearest neighbors f1 score:  0.5711107000399731\n",
      "decision tree train accuracy:  0.9919914575547251\n",
      "decision tree test accuracy:  0.677742193755004\n",
      "decision tree f1 score:  0.578309096837984\n",
      "svm train accuracy:  0.7354511478910838\n",
      "svm test accuracy:  0.754603682946357\n",
      "svm f1 score:  0.4300707278120009\n",
      "CATS: Choose svm for highest test accuracy 0.7546 (f1 score 0.430), choose neighest neighbors for best balance w/ f1 score 0.741 test accuracy (f1 score 0.571)\n"
     ]
    }
   ],
   "source": [
    "# COMPARE MODELS FOR CATS\n",
    "print(\"logistic regression train accuracy: \", lr_train_accuracy_cats)\n",
    "print(\"logistic regression test accuracy: \", lr_test_accuracy_cats)\n",
    "print(\"logistic regression f1 score: \", lr_f1_cats)\n",
    "\n",
    "print(\"ridge regression train accuracy: \", rc_train_accuracy_cats)\n",
    "print(\"ridge regression test accuracy: \", rc_test_accuracy_cats)\n",
    "print(\"ridge regression f1 score: \", rc_f1_cats)\n",
    "\n",
    "print(\"nearest neighbors train accuracy: \", nn_train_accuracy_cats)\n",
    "print(\"nearest neighbors test accuracy: \", nn_test_accuracy_cats)\n",
    "print(\"nearest neighbors f1 score: \", nn_f1_cats)\n",
    "\n",
    "print(\"decision tree train accuracy: \", dt_train_accuracy_cats)\n",
    "print(\"decision tree test accuracy: \", dt_test_accuracy_cats)\n",
    "print(\"decision tree f1 score: \", dt_f1_cats)\n",
    "\n",
    "print(\"svm train accuracy: \", svm_train_accuracy_cats)\n",
    "print(\"svm test accuracy: \", svm_test_accuracy_cats)\n",
    "print(\"svm f1 score: \", svm_f1_cats)\n",
    "\n",
    "print(\"CATS: Choose svm for highest test accuracy 0.7546 (f1 score 0.430), choose neighest neighbors for best balance w/ f1 score 0.741 test accuracy (f1 score 0.571)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
